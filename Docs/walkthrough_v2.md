# V2 Learning OS: Repository Architecture & Orchestration

We have successfully completed the fundamental restructure of the Learning OS. The project is now organized into a "Hub" model that cleanly separates code, content, user state, and test inputs.

## ğŸ—ï¸ The New Architecture

```
school/
â”œâ”€â”€ engine/            # Core Python Package
â”‚   â”œâ”€â”€ cli/           # CLI Commands & Logic
â”‚   â”œâ”€â”€ agents/        # LLM Agents (Track, Campaign, Module, Assignment)
â”‚   â”‚   â”œâ”€â”€ prompts/   # Versioned .md prompt templates
â”‚   â”‚   â””â”€â”€ templates/ # Structural YAML schemas
â”‚   â””â”€â”€ schemas/       # Pydantic models (Config, LearnerState)
â”œâ”€â”€ school-content/    # The Curriculum (Git-managed)
â”‚   â””â”€â”€ projects/      # Tracks & Campaigns
â”œâ”€â”€ learners/          # Production Learner State
â”‚   â””â”€â”€ local_user/    # Your 3-tier student_state.yaml
â”œâ”€â”€ benchmarks/        # Testing & Verification
â”‚   â”œâ”€â”€ cases/         # Benchmark definitions (goal + profile)
â”‚   â”œâ”€â”€ profiles/      # Mock learner archetypes (e.g. math_expert)
â”‚   â””â”€â”€ prompts.yaml   # Global prompt version control
â””â”€â”€ runs/              # Isolated execution sandboxes (gitignored)
```

## ğŸš€ Orchestration & Benchmarking

The new `engine.agents.runner` (The Orchestrator) allows you to simulate the entire learning pipeline in isolated sandboxes.

### Running a Benchmark
Execute a benchmark case in an isolated environment:
```bash
python3 -m engine.agents.runner --benchmark --case-file benchmarks/cases/case_b_quadratic_with_exam.yaml track
```

### ğŸ“‹ Rich Audit Logs
Every benchmark run now generates a `run_log.md` in its sandbox folder. This log provides a full trace of:
- **Metadata**: Model used, goal, case file, and output path.
- **Context**: Exactly what Goal and Learner Profile were sent to the LLM.
- **System Prompt**: The specific version of the instructions used.
- **Raw Output**: The full response from the LLM for inspection.

### ğŸ›ï¸ Dynamic Prompt Control
You no longer need to touch Python code to test new prompts. Use `benchmarks/prompts.yaml` to point the system to different template versions globally.

## ğŸ’ Key Features Implemented
- **3-Tier Competence Model**: Enforced distinction between self-reported (T1), practiced (T2), and verified (T3) knowledge.
- **Learner-Aware Planning**: Agents now respect Tier 3 verified knowledge and skip material the learner already knows.
- **Project Isolation**: Benchmarks run in unique `runs/` folders, protecting your production `school-content` and `learners` data.
- **Clean Naming**: Campaigns now follow the `c01_slug` convention automatically.
